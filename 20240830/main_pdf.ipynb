{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PDF_경우"
      ],
      "metadata": {
        "id": "ZHwO4SQLqh52"
      },
      "id": "ZHwO4SQLqh52"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1SKFOD-cX4P",
        "outputId": "040ca82c-20cd-4bc8-840e-75080497d19e"
      },
      "id": "d1SKFOD-cX4P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m677.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed transformers-4.44.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Tell me what type of data is used in this sentence.'  # 질문을 입력하세요\n",
        "pdf_path = '/content/sample_data/paper/journal.pone.0276781.pdf'  # PDF 문서 경로를 입력하세요\n",
        "file_path2 = '/content/sample_data/paper/journal.pone.0276781.txt'  # 텍스트 파일 경로"
      ],
      "metadata": {
        "id": "SKyClYFvllq0"
      },
      "id": "SKyClYFvllq0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Bert"
      ],
      "metadata": {
        "id": "v6VR2UmwlXHM"
      },
      "id": "v6VR2UmwlXHM"
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 1. PDF 문서에서 텍스트 추출\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# 2. mBERT 모델 초기화\n",
        "def initialize_model():\n",
        "    model_name = \"bert-base-multilingual-cased\"  # mBERT 모델\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "# 3. 프롬프트 엔지니어링 및 질문에 대한 응답 생성\n",
        "# 3. 프롬프트 엔지니어링 및 질문에 대한 응답 생성\n",
        "def answer_question(question, context, tokenizer, model, max_length=512): # add max_length\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt',\n",
        "                                   max_length=max_length, truncation=True) # add truncation\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    answer_tokens = input_ids[0][start_index:end_index + 1]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# 예제 사용\n",
        "\n",
        "context = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "answer = answer_question(question, context, tokenizer, model)\n",
        "\n",
        "print(f'질문: {question}')\n",
        "print(f'답변: {answer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "b1K8MMwGZ6bF",
        "outputId": "136527b6-099f-4efa-af74-153ee6f6f650"
      },
      "id": "b1K8MMwGZ6bF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fitz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f7dfdd6289ce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. PDF 문서에서 텍스트 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. LongformerTokenizer"
      ],
      "metadata": {
        "id": "Cn4ZK8z7mKln"
      },
      "id": "Cn4ZK8z7mKln"
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 1. PDF 문서에서 텍스트 추출 함수\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# 2. 질문에 대한 답변을 추출하는 함수\n",
        "def answer_question(context, question, model, tokenizer):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=4096, truncation=True)\n",
        "    input_ids = inputs['input_ids'].tolist()[0]\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores) + 1\n",
        "\n",
        "    answer_tokens = input_ids[start_index:end_index]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# 4. PDF에서 텍스트 추출\n",
        "context = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# 5. Longformer 모델과 토크나이저 로드\n",
        "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model = LongformerForQuestionAnswering.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "\n",
        "# 7. 질문에 대한 답변 추출\n",
        "answer = answer_question(context, question, model, tokenizer)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpdZ18AsfUCl",
        "outputId": "64b40dfd-57ab-4ff5-b364-bde3d7de8ce7"
      },
      "id": "TpdZ18AsfUCl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  or ARBs on a range of\n",
            "COVID-19 outcomes, including severe disease and concluded that drugs targeting the RAAS\n",
            "should continue to be used [14, 15]. Moreover, in observational studies focusing on hyperten-\n",
            "sive populations, no evidence was found of any worsening effect of ACEi or ARBs on out-\n",
            "comes of COVID-19, and that they may even be protective. However, the majority of these\n",
            "studies compared individuals taking ACEi/ARBs with those not on any antihypertensive medi-\n",
            "cations [16–20]. Nevertheless, the BRACE-CORONA trial, which was the first randomised\n",
            "controlled trial examining the use of ACEi and ARBs in COVID-19 patients concluded that\n",
            "cardiac patients hospitalised with COVID-19 can safely continue taking these drugs [21]. The\n",
            "aim of this study was to estimate the risk of severe COVID-19 between individuals with and\n",
            "without hypertension and explore whether any increased risk of severe COVID-19 was depen-\n",
            "dent on the absolute BP and/or type of antihypertensive treatment.\n",
            "Methods\n",
            "Population and study design\n",
            "Participants from the UK Biobank, a large population-based cohort study, with linked primary health\n",
            "care records, death records and COVID-19 test results were included in these analyses. We used 04/\n",
            "04/2021 as the censor date for all records and test results used for these analyses. The Strengthening\n",
            "the Reporting of Observational Studies in Epidemiology (STROBE) recommendations were used to\n",
            "guide the reporting of this analysis and the STROBE table made publically available.\n",
            "PLOS ONE\n",
            "Primary hypertension and the risk of severe COVID-19\n",
            "PLOS ONE | https\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 1. 텍스트 파일에서 텍스트 읽기 함수\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# 2. Longformer 모델 초기화 함수\n",
        "def initialize_model():\n",
        "    model_name = \"allenai/longformer-base-4096\"  # Longformer 모델\n",
        "    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "    model = LongformerForQuestionAnswering.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "# 3. 질문에 대한 응답 생성 함수\n",
        "def answer_question(question, context, tokenizer, model, max_length=4096):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt',\n",
        "                                   max_length=max_length, truncation=True, padding=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Longformer에서는 `attention_mask`가 필요합니다\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    answer_tokens = input_ids[0][start_index:end_index + 1]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# 예제 사용\n",
        "#file_path2 = '/content/sample_data/paper/journal.pone.0276781.txt'  # 텍스트 파일 경로\n",
        "question = 'What do you use your data for for your experiments?'  # 질문\n",
        "\n",
        "# 텍스트 파일에서 텍스트 읽기\n",
        "context = read_text_file(file_path2)\n",
        "\n",
        "# 모델과 토크나이저 초기화\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# 질문에 대한 답변 생성\n",
        "answer = answer_question(question, context, tokenizer, model)\n",
        "\n",
        "print(f'질문: {question}')\n",
        "print(f'답변: {answer}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FifvLR8tdIl",
        "outputId": "fdaa9bb0-6a73-45db-8c99-bdebcfb3ba41"
      },
      "id": "6FifvLR8tdIl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: What do you use your data for for your experiments?\n",
            "답변:  was dichotomised into white and non-white (due to\n",
            "small numbers) and sex was taken from the baseline UK Biobank visit. All data were assumed\n",
            "to be true at the analysis baseline. Comorbidity was defined as any of the following: peripheral\n",
            "vascular disease (PVD), myocardial infarction (MI), coronary heart disease (CHD) (excluding\n",
            "hypertension), angina, heart failure (HF) and arrhythmia or stroke at any one of the UK Biobank follow-up visits, prevalent at baseline or history of a CV comorbidity based on International Classification of Diseases 10 (ICD10) codes from hospital inpatient episodes or from\n",
            "primary health care records using primary or secondary positions of the diagnostic codes.\n",
            "Death due to COVID-19 was obtained using ICD10 codes from the death records, primary\n",
            "care records and hospital inpatient records. Further information including ICD10 code lists is\n",
            "included in the supplement.\n",
            "Medications were obtained from the UK Biobank database, defined using British National\n",
            "Formulary (BNF) codes and from primary care records using dictionary of medicines and\n",
            "devices (dm+d) codes and local healthcare codes (TPP/EMIS). Relevant medications included\n",
            "ACEi, ARBs and other anti-hypertensives, as well as statins, which are commonly taken in\n",
            "combination with anti-hypertensive medications. All antihypertensive medications not\n",
            "defined as ACEi or ARBs were defined as ‘other’. All medication lists were checked by a\n",
            "clinician. Further information on specific medication lists included in these analyses and code\n",
            "lists can be found in the supplement.\n",
            "COVID-19 data\n",
            "From March 16th 2020, COVID-19 test results from SARS-COV2 polymerase chain reaction\n",
            "(PCR)-based swab tests, provided by Public Health England\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ta67Ft3TKv-g"
      },
      "id": "Ta67Ft3TKv-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. BigBird_안됨\n"
      ],
      "metadata": {
        "id": "M6OCNPH1qpyK"
      },
      "id": "M6OCNPH1qpyK"
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import BigBirdTokenizer, BigBirdForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 1. PDF 문서에서 텍스트 추출 함수\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# 2. BigBird 모델 초기화 함수\n",
        "def initialize_model():\n",
        "    model_name = \"google/bigbird-roberta-base\"  # BigBird 모델\n",
        "    tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "    model = BigBirdForQuestionAnswering.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "# 3. 질문에 대한 응답 생성 함수\n",
        "def answer_question(question, context, tokenizer, model, max_length=4096):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt',\n",
        "                                   max_length=max_length, truncation=True, padding=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    answer_tokens = input_ids[0][start_index:end_index + 1]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# 예제 사용\n",
        "#pdf_path = '/content/sample_data/paper/journal.pone.0276781.pdf'  # PDF 문서 경로\n",
        "#question = 'Please provide detailed information on the data used in the study.'  # 질문\n",
        "\n",
        "# PDF에서 텍스트 추출\n",
        "context = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# 모델과 토크나이저 초기화\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# 질문에 대한 답변 생성\n",
        "answer = answer_question(question, context, tokenizer, model)\n",
        "\n",
        "print(f'질문: {question}')\n",
        "print(f'답변: {answer}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crf6KZZLgas8",
        "outputId": "4c7b61e5-cdfa-4cf2-a2e6-bd44fc1fcd2d"
      },
      "id": "crf6KZZLgas8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForQuestionAnswering were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['qa_classifier.intermediate.dense.bias', 'qa_classifier.intermediate.dense.weight', 'qa_classifier.output.LayerNorm.bias', 'qa_classifier.output.LayerNorm.weight', 'qa_classifier.output.dense.bias', 'qa_classifier.output.dense.weight', 'qa_classifier.qa_outputs.bias', 'qa_classifier.qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: Tell me what type of data is used in this sentence.\n",
            "답변: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. openAI"
      ],
      "metadata": {
        "id": "WmhumpYYs9or"
      },
      "id": "WmhumpYYs9or"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpWxkYXLs82D"
      },
      "id": "rpWxkYXLs82D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정 ##\n",
        "openai.api_key = 'your-openai-api-key'\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    PDF 문서에서 텍스트를 추출하는 함수\n",
        "    \"\"\"\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def get_gpt3_answer(prompt):\n",
        "    \"\"\"\n",
        "    GPT-3 모델을 사용하여 프롬프트에 대한 답변을 생성하는 함수\n",
        "    \"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # GPT-3.5 turbo 모델 사용\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150,  # 응답 길이 제한\n",
        "        n=1,  # 응답 개수\n",
        "        stop=None,\n",
        "        temperature=0.5  # 생성된 텍스트의 다양성\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "# PDF 파일 경로\n",
        "pdf_path = '/content/sample_data/paper/journal.pone.0276781.pdf'\n",
        "\n",
        "# PDF에서 텍스트 추출\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# GPT-3 프롬프트 구성\n",
        "question = \"What's the title?\"\n",
        "prompt = f\"Context: {text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "# GPT-3 모델에 질문하여 답변 추출\n",
        "answer = get_gpt3_answer(prompt)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "D_Olsq4dicho",
        "outputId": "ca29acd4-c06d-49e7-c538-b9c2648b1420"
      },
      "id": "D_Olsq4dicho",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8f60bbb70109>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# GPT-3 모델에 질문하여 답변 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gpt3_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-8f60bbb70109>\u001b[0m in \u001b[0;36mget_gpt3_answer\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mGPT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0m모델을\u001b[0m \u001b[0m사용하여\u001b[0m \u001b[0m프롬프트에\u001b[0m \u001b[0m대한\u001b[0m \u001b[0m답변을\u001b[0m \u001b[0m생성하는\u001b[0m \u001b[0m함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 최신 GPT-3.5 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b989f430c4c3918",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-03T01:26:39.946527Z",
          "start_time": "2024-08-03T01:04:40.977968Z"
        },
        "id": "6b989f430c4c3918"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ca01fab05c45cd",
      "metadata": {
        "id": "97ca01fab05c45cd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}